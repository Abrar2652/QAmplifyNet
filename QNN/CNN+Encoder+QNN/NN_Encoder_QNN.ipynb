{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a652d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     E:\\Softwares\\Anaconda\n",
      "quantum               *  E:\\Softwares\\Anaconda\\envs\\quantum\n",
      "                         E:\\Softwares\\Miniconda\n",
      "                         E:\\Softwares\\Orange\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a0617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import davies_bouldin_score, pairwise_distances, silhouette_samples, silhouette_score\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "import matplotlib.cm as cm\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a372ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"D:/Research/SC-ML/Model/best vif without sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d311e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"x_vif_only.csv\")\n",
    "X.drop(['Unnamed: 0','rev_stop'], axis=1, inplace=True)\n",
    "y = pd.read_csv(\"y.csv\")\n",
    "df=pd.concat([X,y],axis=1)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aaecbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No backorder 99.3 % of the dataset\n",
      "Backorder 0.7 % of the dataset\n",
      "Train: [ 259572  259620  259654 ... 1775315 1775316 1775317] Test: [     0      1      2 ... 355895 355896 355897]\n",
      "Train: [      0       1       2 ... 1775315 1775316 1775317] Test: [259572 259620 259654 ... 711229 711230 711231]\n",
      "Train: [      0       1       2 ... 1775315 1775316 1775317] Test: [ 562975  562992  563216 ... 1066527 1066528 1066529]\n",
      "Train: [      0       1       2 ... 1775315 1775316 1775317] Test: [ 913555  913679  913781 ... 1549977 1550291 1550807]\n",
      "Train: [      0       1       2 ... 1549977 1550291 1550807] Test: [1420136 1420137 1420138 ... 1775315 1775316 1775317]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Label Distributions: \n",
      "\n",
      "[0.99296253 0.00703747]\n",
      "[0.99296181 0.00703819]\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"x_vif_only.csv\")\n",
    "X.drop(['Unnamed: 0','rev_stop'], axis=1, inplace=True)\n",
    "y = pd.read_csv(\"y.csv\", index_col=None)['went_on_backorder']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "print('No backorder', round(y.value_counts()[0]/len(y) * 100,2), '% of the dataset')\n",
    "print('Backorder', round(y.value_counts()[1]/len(y) * 100,2), '% of the dataset')\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain))\n",
    "print(test_counts_label/ len(original_ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18a1669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X (train): 1420255 | Length of y (train): 1420255\n",
      "Length of X (test): 355063 | Length of y (test): 355063\n",
      "Before UnderSampling, counts of label '1': 9995\n",
      "Before UnderSampling, counts of label '0': 1410260 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
    "print(\"Before UnderSampling, counts of label '1': {}\".format(sum(original_ytrain == 1)))\n",
    "print(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(original_ytrain == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f130fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test, counts of label '1': 2499\n",
      "test, counts of label '0': 352564 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"test, counts of label '1': {}\".format(sum(original_ytest == 1)))\n",
    "print(\"test, counts of label '0': {} \\n\".format(sum(original_ytest == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5649647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "sm1 = NearMiss(sampling_strategy={0:500,1:500}) # 500:500\n",
    "Xsm_train, ysm_train = sm1.fit_resample(original_Xtrain, original_ytrain)\n",
    "sm2 = NearMiss(sampling_strategy={0:200,1:67}) # 200:200\n",
    "Xsm_test, ysm_test = sm2.fit_resample(original_Xtest, original_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a14515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Sampling, the shape of train_X: (1000, 14)\n",
      "After Sampling, the shape of train_y: (1000,) \n",
      "\n",
      "After Sampling, counts of label '1': 500\n",
      "After Sampling, counts of label '0': 500\n"
     ]
    }
   ],
   "source": [
    "print('After Sampling, the shape of train_X: {}'.format(Xsm_train.shape))\n",
    "print('After Sampling, the shape of train_y: {} \\n'.format(ysm_train.shape))\n",
    "\n",
    "print(\"After Sampling, counts of label '1': {}\".format(sum(ysm_train == 1)))\n",
    "print(\"After Sampling, counts of label '0': {}\".format(sum(ysm_train == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b35da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.881832</td>\n",
       "      <td>-1.584418</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>-0.571375</td>\n",
       "      <td>-0.947434</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.881832</td>\n",
       "      <td>-1.584418</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>-0.571375</td>\n",
       "      <td>-0.947434</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.881832</td>\n",
       "      <td>-1.584418</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>-0.571375</td>\n",
       "      <td>-0.947434</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.179825</td>\n",
       "      <td>-1.231264</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>-0.571375</td>\n",
       "      <td>-0.661681</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.179825</td>\n",
       "      <td>-1.231264</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>-0.571375</td>\n",
       "      <td>-0.661681</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-1.838447</td>\n",
       "      <td>-1.584418</td>\n",
       "      <td>-1.488533</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>0.213868</td>\n",
       "      <td>-0.947434</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>2.715014</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.837957</td>\n",
       "      <td>-1.024682</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>0.302822</td>\n",
       "      <td>-0.041619</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.478307</td>\n",
       "      <td>-0.671527</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>0.378032</td>\n",
       "      <td>0.468242</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-2.847583</td>\n",
       "      <td>-1.584418</td>\n",
       "      <td>-1.488533</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>0.443182</td>\n",
       "      <td>-0.661681</td>\n",
       "      <td>-0.665307</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.925708</td>\n",
       "      <td>-0.671527</td>\n",
       "      <td>0.344253</td>\n",
       "      <td>-0.43232</td>\n",
       "      <td>-0.571375</td>\n",
       "      <td>-0.283936</td>\n",
       "      <td>-0.257233</td>\n",
       "      <td>-0.105932</td>\n",
       "      <td>-0.084376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2        3         4         5         6  \\\n",
       "0    0.881832 -1.584418  0.344253 -0.43232 -0.571375 -0.947434 -0.665307   \n",
       "1    0.881832 -1.584418  0.344253 -0.43232 -0.571375 -0.947434 -0.665307   \n",
       "2    0.881832 -1.584418  0.344253 -0.43232 -0.571375 -0.947434 -0.665307   \n",
       "3    0.179825 -1.231264  0.344253 -0.43232 -0.571375 -0.661681 -0.665307   \n",
       "4    0.179825 -1.231264  0.344253 -0.43232 -0.571375 -0.661681 -0.665307   \n",
       "..        ...       ...       ...      ...       ...       ...       ...   \n",
       "995 -1.838447 -1.584418 -1.488533 -0.43232  0.213868 -0.947434 -0.665307   \n",
       "996  0.837957 -1.024682  0.344253 -0.43232  0.302822 -0.041619 -0.665307   \n",
       "997 -0.478307 -0.671527  0.344253 -0.43232  0.378032  0.468242 -0.665307   \n",
       "998 -2.847583 -1.584418 -1.488533 -0.43232  0.443182 -0.661681 -0.665307   \n",
       "999  0.925708 -0.671527  0.344253 -0.43232 -0.571375 -0.283936 -0.257233   \n",
       "\n",
       "            7         8    9   10   11   12   13  \n",
       "0   -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "1   -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "2   -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "3   -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "4   -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "..        ...       ...  ...  ...  ...  ...  ...  \n",
       "995  2.715014 -0.084376  0.0  0.0  0.0  1.0  1.0  \n",
       "996 -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "997 -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "998 -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "999 -0.105932 -0.084376  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[1000 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsm_train_df, ysm_train_df = pd.DataFrame(Xsm_train), pd.DataFrame(ysm_train)\n",
    "Xsm_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d7a1b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(.96)\n",
    "pca.fit(Xsm_train_df)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c0420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_Xtest_df = pd.DataFrame(Xsm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1708c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsm_train_df_pca = pd.DataFrame(pca.transform(Xsm_train_df))\n",
    "original_Xtest_df_pca = pd.DataFrame(pca.transform(original_Xtest_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2ec116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsm_train_df_pca.to_csv('Xmiss_train_pca.csv',header=False,index=False)\n",
    "ysm_train_df.to_csv('ymiss_train_pca.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ede4c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtest_df_pca = pd.concat([original_Xtest_df_pca, pd.DataFrame(ysm_test)], axis=1)\n",
    "qtest_df_pca.to_csv('qtest_df_pca.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdf498eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20ea32a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_combined)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eeedf8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.32931412  0.1654671  -0.21998954 ... -0.02012723  0.00725466\n",
      "   0.        ]\n",
      " [-1.32931412  0.1654671  -0.21998954 ... -0.02012723  0.00725466\n",
      "   0.        ]\n",
      " [-1.32931412  0.1654671  -0.21998954 ... -0.02012723  0.00725466\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.03520317  0.7272464   0.62547005 ...  0.17382669 -0.41565043\n",
      "   1.        ]\n",
      " [ 2.83593721  0.24738456 -0.13918545 ... -0.09135539  0.05837374\n",
      "   1.        ]\n",
      " [-1.41839267  0.13190045 -0.14533605 ... -0.01848664  0.15389684\n",
      "   1.        ]]\n",
      "done1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Fitting script\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, BSgate, Kgate, Sgate, Rgate\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Hyperparameters\n",
    "# ===================================================================================\n",
    "\n",
    "# Two modes required: one for \"genuine\" transactions and one for \"fradulent\"\n",
    "mode_number = 2\n",
    "# Number of photonic quantum layers\n",
    "depth = 4\n",
    "\n",
    "# Fock basis truncation\n",
    "cutoff = 10\n",
    "# Number of batches in optimization\n",
    "reps = 30000 #30000\n",
    "\n",
    "# Label for simulation\n",
    "simulation_label = 1\n",
    "\n",
    "# Number of batches to use in the optimization\n",
    "batch_size = 5 #factor of the total rows # it was 24\n",
    "\n",
    "# Random initialization of gate parameters\n",
    "sdev_photon = 0.1\n",
    "sdev = 1\n",
    "\n",
    "# Variable clipping values\n",
    "disp_clip = 5\n",
    "sq_clip = 5\n",
    "kerr_clip = 1\n",
    "\n",
    "# If loading from checkpoint, previous batch number reached\n",
    "ckpt_val = 0\n",
    "\n",
    "# Number of repetitions between each output to TensorBoard\n",
    "tb_reps = 100\n",
    "# Number of repetitions between each model save\n",
    "savr_reps = 1000\n",
    "\n",
    "model_string = str(simulation_label)\n",
    "\n",
    "# Target location of output\n",
    "folder_locator = './outputs/'\n",
    "\n",
    "# Locations of TensorBoard and model save outputs\n",
    "board_string = folder_locator + 'tensorboard/' + model_string + '/'\n",
    "checkpoint_string = folder_locator + 'models/' + model_string + '/'\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Loading the training data\n",
    "# ===================================================================================\n",
    "\n",
    "# Combining genuine and fraudulent data\n",
    "data_Xsm_train = np.loadtxt('Xmiss_train_pca.csv', delimiter=',')\n",
    "data_ysm_train = np.loadtxt('ymiss_train_pca.csv', delimiter=',')\n",
    "\n",
    "# Combining genuine and fraudulent data\n",
    "data_combined = np.column_stack([data_Xsm_train, data_ysm_train])\n",
    "data_points = len(data_combined)\n",
    "print(data_combined)\n",
    "# ===================================================================================\n",
    "#                                   Setting up the classical NN input\n",
    "# ===================================================================================\n",
    "\n",
    "# Input neurons\n",
    "input_neurons = 8  ## number of features\n",
    "# Widths of hidden layers\n",
    "nn_architecture = [10, 10] \n",
    "# Output neurons of classical part\n",
    "output_neurons = 14\n",
    "\n",
    "# Defining classical network parameters\n",
    "#input_classical_layer = tf.placeholder(tf.float32, shape=[batch_size, input_neurons])\n",
    "input_classical_layer = tf.placeholder(tf.float32, shape=[batch_size, input_neurons])\n",
    "\n",
    "layer_matrix_1 = tf.Variable(tf.random_normal(shape=[input_neurons, nn_architecture[0]]))\n",
    "offset_1 = tf.Variable(tf.random_normal(shape=[nn_architecture[0]]))\n",
    "\n",
    "layer_matrix_2 = tf.Variable(tf.random_normal(shape=[nn_architecture[0], nn_architecture[1]]))\n",
    "offset_2 = tf.Variable(tf.random_normal(shape=[nn_architecture[1]]))\n",
    "\n",
    "layer_matrix_3 = tf.Variable(tf.random_normal(shape=[nn_architecture[1], output_neurons]))\n",
    "offset_3 = tf.Variable(tf.random_normal(shape=[output_neurons]))\n",
    "\n",
    "# Creating hidden layers and output\n",
    "layer_1 = tf.nn.elu(tf.matmul(input_classical_layer, layer_matrix_1) + offset_1)\n",
    "layer_2 = tf.nn.elu(tf.matmul(layer_1, layer_matrix_2) + offset_2)\n",
    "\n",
    "output_layer = tf.nn.elu(tf.matmul(layer_2, layer_matrix_3) + offset_3)\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Defining QNN parameters\n",
    "# ===================================================================================\n",
    "\n",
    "# Number of beamsplitters in interferometer\n",
    "bs_in_interferometer = int(1.0 * mode_number * (mode_number - 1) / 2)\n",
    "\n",
    "with tf.name_scope('variables'):\n",
    "    bs_variables = tf.Variable(tf.random_normal(shape=[depth, bs_in_interferometer, 2, 2]\n",
    "                                                , stddev=sdev))\n",
    "    phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number, 2], stddev=sdev))\n",
    "\n",
    "    sq_magnitude_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                          , stddev=sdev_photon))\n",
    "    sq_phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                      , stddev=sdev))\n",
    "    disp_magnitude_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                            , stddev=sdev_photon))\n",
    "    disp_phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                        , stddev=sdev))\n",
    "    kerr_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number], stddev=sdev_photon))\n",
    "\n",
    "parameters = [layer_matrix_1, offset_1, layer_matrix_2, offset_2, layer_matrix_3, offset_3, bs_variables,\n",
    "              phase_variables, sq_magnitude_variables, sq_phase_variables, disp_magnitude_variables,\n",
    "              disp_phase_variables, kerr_variables]\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Constructing quantum layers\n",
    "# ===================================================================================\n",
    "\n",
    "\n",
    "# Defining input QNN layer, whose parameters are set by the outputs of the classical network\n",
    "def input_qnn_layer():\n",
    "    with tf.name_scope('inputlayer'):\n",
    "        Sgate(tf.clip_by_value(output_layer[:, 0], -sq_clip, sq_clip), output_layer[:, 1]) | q[0]\n",
    "        Sgate(tf.clip_by_value(output_layer[:, 2], -sq_clip, sq_clip), output_layer[:, 3]) | q[1]\n",
    "\n",
    "        BSgate(output_layer[:, 4], output_layer[:, 5]) | (q[0], q[1])\n",
    "\n",
    "        Rgate(output_layer[:, 6]) | q[0]\n",
    "        Rgate(output_layer[:, 7]) | q[1]\n",
    "\n",
    "        Dgate(tf.clip_by_value(output_layer[:, 8], -disp_clip, disp_clip), output_layer[:, 9]) \\\n",
    "        | q[0]\n",
    "        Dgate(tf.clip_by_value(output_layer[:, 10], -disp_clip, disp_clip), output_layer[:, 11]) \\\n",
    "        | q[1]\n",
    "\n",
    "        Kgate(tf.clip_by_value(output_layer[:, 12], -kerr_clip, kerr_clip)) | q[0]\n",
    "        Kgate(tf.clip_by_value(output_layer[:, 13], -kerr_clip, kerr_clip)) | q[1]\n",
    "\n",
    "\n",
    "# Defining standard QNN layers\n",
    "def qnn_layer(layer_number):\n",
    "    with tf.name_scope('layer_{}'.format(layer_number)):\n",
    "        BSgate(bs_variables[layer_number, 0, 0, 0], bs_variables[layer_number, 0, 0, 1]) \\\n",
    "        | (q[0], q[1])\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Rgate(phase_variables[layer_number, i, 0]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Sgate(tf.clip_by_value(sq_magnitude_variables[layer_number, i], -sq_clip, sq_clip),\n",
    "                  sq_phase_variables[layer_number, i]) | q[i]\n",
    "\n",
    "        BSgate(bs_variables[layer_number, 0, 1, 0], bs_variables[layer_number, 0, 1, 1]) \\\n",
    "        | (q[0], q[1])\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Rgate(phase_variables[layer_number, i, 1]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Dgate(tf.clip_by_value(disp_magnitude_variables[layer_number, i], -disp_clip,\n",
    "                                   disp_clip), disp_phase_variables[layer_number, i]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Kgate(tf.clip_by_value(kerr_variables[layer_number, i], -kerr_clip, kerr_clip)) | q[i]\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Defining QNN\n",
    "# ===================================================================================\n",
    "\n",
    "# construct the two-mode Strawberry Fields engine\n",
    "eng, q = sf.Engine(mode_number)\n",
    "\n",
    "# construct the circuit\n",
    "with eng:\n",
    "    input_qnn_layer()\n",
    "    for i in range(depth):\n",
    "        qnn_layer(i)\n",
    "\n",
    "# run the engine (in batch mode)\n",
    "state = eng.run(\"tf\", cutoff_dim=cutoff, eval=False, batch_size=batch_size)\n",
    "# extract the state\n",
    "ket = state.ket()\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Setting up cost function\n",
    "# ===================================================================================\n",
    "\n",
    "# Classifications for whole batch: rows act as data points in the batch and columns\n",
    "# are the one-hot classifications\n",
    "classification = tf.placeholder(shape=[batch_size, 2], dtype=tf.int32)\n",
    "\n",
    "func_to_minimise = 0\n",
    "\n",
    "# Building up the function to minimize by looping through batch\n",
    "for i in range(batch_size):\n",
    "    # Probabilities corresponding to a single photon in either mode\n",
    "    prob = tf.abs(ket[i, classification[i, 0], classification[i, 1]]) ** 2\n",
    "    # These probabilities should be optimised to 1\n",
    "    func_to_minimise += (1.0 / batch_size) * (prob - 1) ** 2\n",
    "\n",
    "# Defining the cost function\n",
    "cost_func = func_to_minimise\n",
    "tf.summary.scalar('Cost', cost_func)\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Training\n",
    "# ===================================================================================\n",
    "\n",
    "# We choose the Adam optimizer\n",
    "optimiser = tf.train.AdamOptimizer()\n",
    "training = optimiser.minimize(cost_func)\n",
    "\n",
    "# Saver/Loader for outputting model\n",
    "saver = tf.train.Saver(parameters)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Load previous model if non-zero ckpt_val is specified\n",
    "if ckpt_val != 0:\n",
    "    saver.restore(session, checkpoint_string + 'sess.ckpt-' + str(ckpt_val))\n",
    "\n",
    "# TensorBoard writer\n",
    "writer = tf.summary.FileWriter(board_string)\n",
    "merge = tf.summary.merge_all()\n",
    "\n",
    "counter = ckpt_val\n",
    "\n",
    "# Tracks optimum value found (set high so first iteration encodes value)\n",
    "opt_val = 1e20\n",
    "# Batch number in which optimum value occurs\n",
    "opt_position = 0\n",
    "# Flag to detect if new optimum occured in last batch\n",
    "new_opt = False\n",
    "print('done1')\n",
    "while counter <= reps:\n",
    "    # Shuffles data to create new epoch\n",
    "    np.random.shuffle(data_combined)\n",
    "\n",
    "    # Splits data into batches    \n",
    "    split_data = np.split(data_combined, data_points / batch_size) #equal divisions, array_split->nearly equal divisions\n",
    "\n",
    "    for batch in split_data:\n",
    "        # there are arrays with shape(40,14) which can't be reshaped into (40,14). So, filtering (40,14) arrays\n",
    "        if counter > reps:\n",
    "            break\n",
    "        # Input data (provided as principal components)\n",
    "        data_points_principal_components = batch[:, :input_neurons] #batch[:, 1:input_neurons+1] if there are indices in col1\n",
    "        #data_points_principal_components = data_points_principal_components.astype(np.float32)\n",
    "        \n",
    "        # Data classes\n",
    "        classes = batch[:, -1]\n",
    "\n",
    "        # Encoding classes into one-hot form\n",
    "        one_hot_input = np.zeros((batch_size, 2), np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            if int(classes[i]) == 0:\n",
    "                # Encoded such that genuine transactions should be outputted as a photon in the first mode\n",
    "                one_hot_input[i] = [1, 0]\n",
    "            else:\n",
    "                one_hot_input[i] = [0, 1]\n",
    "\n",
    "        #one_hot_input = one_hot_input.astype(np.int32)\n",
    "\n",
    "        # Output to TensorBoard\n",
    "        if counter % tb_reps == 0:\n",
    "            [summary, training_run, func_to_minimise_run] = session.run([merge, training, func_to_minimise],\n",
    "                                                                        feed_dict={\n",
    "                                                                            input_classical_layer:\n",
    "                                                                                data_points_principal_components,\n",
    "                                                                            classification: one_hot_input})\n",
    "            writer.add_summary(summary, counter)\n",
    "\n",
    "        else:\n",
    "            # Standard run of training\n",
    "            [training_run, func_to_minimise_run] = session.run([training, func_to_minimise], feed_dict={\n",
    "                input_classical_layer: data_points_principal_components, classification: one_hot_input})\n",
    "\n",
    "        # Ensures cost function is well behaved\n",
    "        if np.isnan(func_to_minimise_run):\n",
    "            compute_grads = session.run(optimiser.compute_gradients(cost_func),\n",
    "                                        feed_dict={input_classical_layer: data_points_principal_components,\n",
    "                                                   classification: one_hot_input})\n",
    "            if not os.path.exists(checkpoint_string):\n",
    "                os.makedirs(checkpoint_string)\n",
    "            # If cost function becomes NaN, output value of gradients for investigation\n",
    "            np.save(checkpoint_string + 'NaN.npy', compute_grads)\n",
    "            print('NaNs outputted - leaving at step ' + str(counter))\n",
    "            raise SystemExit\n",
    "\n",
    "        # Test to see if new optimum found in current batch\n",
    "        if func_to_minimise_run < opt_val:\n",
    "            opt_val = func_to_minimise_run\n",
    "            opt_position = counter\n",
    "            new_opt = True\n",
    "\n",
    "        # Save model every fixed number of batches, provided a new optimum value has occurred\n",
    "        if (counter % savr_reps == 0) and (i != 0) and new_opt and (not np.isnan(func_to_minimise_run)):\n",
    "            if not os.path.exists(checkpoint_string):\n",
    "                os.makedirs(checkpoint_string)\n",
    "            saver.save(session, checkpoint_string + 'sess.ckpt', global_step=counter)\n",
    "            # Saves position of optimum and corresponding value of cost function\n",
    "            np.savetxt(checkpoint_string + 'optimum.txt', [opt_position, opt_val])\n",
    "\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6053e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a391b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "996b53e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "267/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b9df54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./outputs/models/1/sess.ckpt-30000\n",
      "done1\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2018 Xanadu Quantum Technologies Inc.\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Fraud detection fitting script\"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, BSgate, Kgate, Sgate, Rgate\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Hyperparameters\n",
    "# ===================================================================================\n",
    "\n",
    "# Two modes required: one for \"genuine\" transactions and one for \"fradulent\"\n",
    "mode_number = 2\n",
    "# Number of photonic quantum layers\n",
    "depth = 4\n",
    "\n",
    "# Fock basis truncation\n",
    "cutoff = 10\n",
    "\n",
    "# Label for simulation\n",
    "simulation_label = 1\n",
    "\n",
    "# Random initialization of gate parameters\n",
    "sdev_photon = 0.1\n",
    "sdev = 1\n",
    "\n",
    "# Variable clipping values\n",
    "disp_clip = 5\n",
    "sq_clip = 5\n",
    "kerr_clip = 1\n",
    "\n",
    "# If loading from checkpoint, previous batch number reached\n",
    "ckpt_val = 30000 #30000\n",
    "\n",
    "model_string = str(simulation_label)\n",
    "\n",
    "# Target location of output\n",
    "folder_locator = './outputs/'\n",
    "\n",
    "# Locations of model saves and where confusion matrix will be saved\n",
    "checkpoint_string = folder_locator + 'models/' + model_string + '/'\n",
    "confusion_string = folder_locator + 'confusion/' + model_string + '/'\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Loading the testing data\n",
    "# ===================================================================================\n",
    "\n",
    "# Loading combined dataset with extra genuine datapoints unseen in training\n",
    "data_combined = np.loadtxt('qtest_df_pca.csv', delimiter=',')[:]\n",
    "\n",
    "# Set to a size so that the data can be equally split up with no remainder\n",
    "batch_size = 3 #constant for qtest_df data\n",
    "\n",
    "data_combined_points = len(data_combined)\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Setting up the classical NN input\n",
    "# ===================================================================================\n",
    "\n",
    "# Input neurons\n",
    "input_neurons = 8\n",
    "# Widths of hidden layers\n",
    "nn_architecture = [10, 10]\n",
    "# Output neurons of classical part\n",
    "output_neurons = 14\n",
    "\n",
    "# Defining classical network parameters\n",
    "input_classical_layer = tf.placeholder(tf.float32, shape=[batch_size, input_neurons])\n",
    "\n",
    "layer_matrix_1 = tf.Variable(tf.random_normal(shape=[input_neurons, nn_architecture[0]]))\n",
    "offset_1 = tf.Variable(tf.random_normal(shape=[nn_architecture[0]]))\n",
    "\n",
    "layer_matrix_2 = tf.Variable(tf.random_normal(shape=[nn_architecture[0], nn_architecture[1]]))\n",
    "offset_2 = tf.Variable(tf.random_normal(shape=[nn_architecture[1]]))\n",
    "\n",
    "layer_matrix_3 = tf.Variable(tf.random_normal(shape=[nn_architecture[1], output_neurons]))\n",
    "offset_3 = tf.Variable(tf.random_normal(shape=[output_neurons]))\n",
    "\n",
    "# Creating hidden layers and output\n",
    "layer_1 = tf.nn.elu(tf.matmul(input_classical_layer, layer_matrix_1) + offset_1)\n",
    "layer_2 = tf.nn.elu(tf.matmul(layer_1, layer_matrix_2) + offset_2)\n",
    "\n",
    "output_layer = tf.nn.elu(tf.matmul(layer_2, layer_matrix_3) + offset_3)\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Defining QNN parameters\n",
    "# ===================================================================================\n",
    "\n",
    "# Number of beamsplitters in interferometer\n",
    "bs_in_interferometer = int(1.0 * mode_number * (mode_number - 1) / 2)\n",
    "\n",
    "with tf.name_scope('variables'):\n",
    "    bs_variables = tf.Variable(tf.random_normal(shape=[depth, bs_in_interferometer, 2, 2]\n",
    "                                                , stddev=sdev))\n",
    "    phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number, 2], stddev=sdev))\n",
    "\n",
    "    sq_magnitude_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                          , stddev=sdev_photon))\n",
    "    sq_phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                      , stddev=sdev))\n",
    "    disp_magnitude_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                            , stddev=sdev_photon))\n",
    "    disp_phase_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number]\n",
    "                                                        , stddev=sdev))\n",
    "    kerr_variables = tf.Variable(tf.random_normal(shape=[depth, mode_number], stddev=sdev_photon))\n",
    "\n",
    "parameters = [layer_matrix_1, offset_1, layer_matrix_2, offset_2, layer_matrix_3, offset_3, bs_variables,\n",
    "              phase_variables, sq_magnitude_variables, sq_phase_variables, disp_magnitude_variables,\n",
    "              disp_phase_variables, kerr_variables]\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Constructing quantum layers\n",
    "# ===================================================================================\n",
    "\n",
    "\n",
    "# Defining input QNN layer, whose parameters are set by the outputs of the classical network\n",
    "def input_qnn_layer():\n",
    "    with tf.name_scope('inputlayer'):\n",
    "        Sgate(tf.clip_by_value(output_layer[:, 0], -sq_clip, sq_clip), output_layer[:, 1]) | q[0]\n",
    "        Sgate(tf.clip_by_value(output_layer[:, 2], -sq_clip, sq_clip), output_layer[:, 3]) | q[1]\n",
    "\n",
    "        BSgate(output_layer[:, 4], output_layer[:, 5]) | (q[0], q[1])\n",
    "\n",
    "        Rgate(output_layer[:, 6]) | q[0]\n",
    "        Rgate(output_layer[:, 7]) | q[1]\n",
    "\n",
    "        Dgate(tf.clip_by_value(output_layer[:, 8], -disp_clip, disp_clip), output_layer[:, 9]) \\\n",
    "        | q[0]\n",
    "        Dgate(tf.clip_by_value(output_layer[:, 10], -disp_clip, disp_clip), output_layer[:, 11]) \\\n",
    "        | q[1]\n",
    "\n",
    "        Kgate(tf.clip_by_value(output_layer[:, 12], -kerr_clip, kerr_clip)) | q[0]\n",
    "        Kgate(tf.clip_by_value(output_layer[:, 13], -kerr_clip, kerr_clip)) | q[1]\n",
    "\n",
    "\n",
    "# Defining standard QNN layers\n",
    "def qnn_layer(layer_number):\n",
    "    with tf.name_scope('layer_{}'.format(layer_number)):\n",
    "        BSgate(bs_variables[layer_number, 0, 0, 0], bs_variables[layer_number, 0, 0, 1]) \\\n",
    "        | (q[0], q[1])\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Rgate(phase_variables[layer_number, i, 0]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Sgate(tf.clip_by_value(sq_magnitude_variables[layer_number, i], -sq_clip, sq_clip),\n",
    "                  sq_phase_variables[layer_number, i]) | q[i]\n",
    "\n",
    "        BSgate(bs_variables[layer_number, 0, 1, 0], bs_variables[layer_number, 0, 1, 1]) \\\n",
    "        | (q[0], q[1])\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Rgate(phase_variables[layer_number, i, 1]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Dgate(tf.clip_by_value(disp_magnitude_variables[layer_number, i], -disp_clip,\n",
    "                                   disp_clip), disp_phase_variables[layer_number, i]) | q[i]\n",
    "\n",
    "        for i in range(mode_number):\n",
    "            Kgate(tf.clip_by_value(kerr_variables[layer_number, i], -kerr_clip, kerr_clip)) | q[i]\n",
    "\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Defining QNN\n",
    "# ===================================================================================\n",
    "\n",
    "# construct the two-mode Strawberry Fields engine\n",
    "eng, q = sf.Engine(mode_number)\n",
    "\n",
    "# construct the circuit\n",
    "with eng:\n",
    "    input_qnn_layer()\n",
    "\n",
    "    for i in range(depth):\n",
    "        qnn_layer(i)\n",
    "\n",
    "# run the engine (in batch mode)\n",
    "state = eng.run(\"tf\", cutoff_dim=cutoff, eval=False, batch_size=batch_size)\n",
    "# extract the state\n",
    "ket = state.ket()\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Extracting probabilities\n",
    "# ===================================================================================\n",
    "\n",
    "# Classifications for whole batch: rows act as data points in the batch and columns\n",
    "# are the one-hot classifications\n",
    "classification = tf.placeholder(shape=[batch_size, 2], dtype=tf.int32)\n",
    "\n",
    "prob = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Finds the probability of a photon being in either mode\n",
    "    prob.append([tf.abs(ket[i, 1, 0]) ** 2, tf.abs(ket[i, 0, 1]) ** 2])\n",
    "\n",
    "# ===================================================================================\n",
    "#                                   Testing performance\n",
    "# ===================================================================================\n",
    "\n",
    "# Defining array of thresholds from 0 to 1 to consider in the ROC curve\n",
    "thresholds_points = 101\n",
    "thresholds = np.linspace(0, 1, num=thresholds_points)\n",
    "\n",
    "# Saver/Loader for outputting model\n",
    "saver = tf.train.Saver(parameters)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "saver.restore(session, checkpoint_string + 'sess.ckpt-' + str(ckpt_val))\n",
    "print('done1')\n",
    "\n",
    "# Split up data to process in batches\n",
    "data_split = np.split(data_combined, data_combined_points / batch_size)\n",
    "\n",
    "# Defining confusion table\n",
    "confusion_table = np.zeros((thresholds_points, 2, 2))\n",
    "\n",
    "for batch in data_split:\n",
    "    # Input data (provided as principal components)\n",
    "    data_points_principal_components = batch[:, 1:input_neurons + 1]\n",
    "    data_points_principal_components = data_points_principal_components.astype(np.float32)\n",
    "    # Data classes\n",
    "    classes = batch[:, -1]\n",
    "    classes = classes.astype(np.int32)\n",
    "    # Probabilities outputted from circuit\n",
    "    prob_run = session.run(prob, feed_dict={input_classical_layer: data_points_principal_components})\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Calculate probabilities of photon coming out of either mode\n",
    "        p = prob_run[i]\n",
    "        # Normalize to these two events (i.e. ignore all other outputs)\n",
    "        p = p / np.sum(p)\n",
    "\n",
    "        # Predicted class is a list corresponding to threshold probabilities\n",
    "        predicted_class = []\n",
    "\n",
    "        for j in range(thresholds_points):\n",
    "            # If probability of a photon exiting first mode is larger than threshold, attribute as genuine\n",
    "            if p[0] > thresholds[j]:\n",
    "                predicted_class.append(0)\n",
    "            else:\n",
    "                predicted_class.append(1)\n",
    "\n",
    "        actual_class = classes[i]\n",
    "\n",
    "        # Constructing confusion table\n",
    "        for j in range(2):\n",
    "            for k in range(2):\n",
    "                for l in range(thresholds_points):\n",
    "                    if actual_class == j and predicted_class[l] == k:\n",
    "                        confusion_table[l, j, k] += 1\n",
    "\n",
    "# Renormalizing confusion table\n",
    "for i in range(thresholds_points):\n",
    "    confusion_table[i] = confusion_table[i] / data_combined_points * 100\n",
    "\n",
    "if not os.path.exists(confusion_string):\n",
    "    os.makedirs(confusion_string)\n",
    "\n",
    "# Save as numpy array\n",
    "np.save(confusion_string + 'confusion_table.npy', confusion_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a36c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
